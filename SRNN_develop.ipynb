{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spiking Recurrent Neural Network (SRNN) for Memtransistor Crossbar Array Simulation**\n",
    "### **Neuron**\n",
    "    - (TODO) Integrate-and-fire (IF) neuron\n",
    "    - (TODO) Leaky integrate-and-fire (LIF) neuron\n",
    "    - (TODO) Adaptive leaky integrate-and-fire (ALIF) neuron\n",
    "### **Weight Update methods**\n",
    "    - (TODO) Back propogation (BP) \n",
    "    - (TODO) Back propagation through time (BPTT) \n",
    "    - (TODO) Spike timing depedent plasticity (STDP)\n",
    "### **Model**\n",
    "    - (TODO) train\n",
    "    - (TODO) predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - **Neuron.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell = tf.keras.layers.AbstractRNNCell\n",
    "IFStateTuple = namedtuple('IFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "LIFStateTuple = namedtuple('LIFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "ALIFStateTuple = namedtuple('ALIFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "class IF(Cell):\n",
    "    def __init__(self, units, thr, capacitance = 1.0, dtype=tf.float32):\n",
    "        self.units = units\n",
    "        self.thr = tf.Variable(thr, dtype=dtype, name=\"Threshold\", trainable=False) \n",
    "        self.capacitance = capacitance\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.units\n",
    "    \n",
    "    def output_size(self):\n",
    "        return self.units\n",
    "\n",
    "    def __call__(self):\n",
    "        self.current = self.units - 1\n",
    "\n",
    "    def neuronal_dynamic(self, v_t, z_t):\n",
    "        \"\"\"\n",
    "        Function that generate the next spike and voltage tensor for given cell state.\n",
    "        :param thr - membrane threshold voltage\n",
    "        :param v - current membrane voltage\n",
    "        :param z - input spike train from previous layer at time t\n",
    "        :return: current v, z\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.units != len(v_t), \"number of neuron should be the same as number of voltage\"\n",
    "\n",
    "        # calculate the new voltages of this layer at time t+1\n",
    "        v_new = np.sum(z_t) * self.capacitance + v_t\n",
    "\n",
    "        z_new = np.zeros(self.units)\n",
    "\n",
    "        # update the spike train of this layer\n",
    "        for i in range(self.units):\n",
    "            if v_new[i] > self.thr:\n",
    "                z_new[i] = 1\n",
    "\n",
    "        return v_new, z_new\n",
    "    \n",
    "\n",
    "\n",
    "class LIF_recurrent(Cell):\n",
    "    def __init__(self, n_in, n_rec, w0, w_min, w_max, a_plus, a_plus_sign, a_minus, a_minus_sign, \n",
    "                 b_plus, b_plus_sign, b_minus, b_minus_sign, \n",
    "                 c_plus, c_plus_sign, c_minus,c_minus_sign, STDP_dev = 0.0, tau=20., thr=0.03,\n",
    "                 dt=1., n_refractory=0, dtype=tf.float32, n_delay=1, rewiring_connectivity=-1,\n",
    "                 in_neuron_sign=None, rec_neuron_sign=None,\n",
    "                 dampening_factor=0.3,\n",
    "                 injected_noise_current=0.,\n",
    "                 V0=1.):\n",
    "        \"\"\"\n",
    "        Tensorflow cell object that simulates a LIF neuron with an approximation of the spike derivatives.\n",
    "        :param n_in: number of input neurons\n",
    "        :param n_rec: number of recurrent neurons\n",
    "        :param w_min to c_minus_sign: STDP weight update parameters\n",
    "        :param STDP_dev: standard deviation to depict the randomness of devices, like memtransistors\n",
    "        :param tau: membrane time constant\n",
    "        :param thr: threshold voltage\n",
    "        :param dt: time step of the simulation\n",
    "        :param n_refractory: number of refractory time steps\n",
    "        :param dtype: data type of the cell tensors\n",
    "        :param n_delay: number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        :param reset: method of resetting membrane potential after spike thr-> by fixed threshold amount, zero-> to zero\n",
    "        \"\"\"\n",
    "\n",
    "        if np.isscalar(tau): tau = tf.ones(n_rec, dtype=dtype) * np.mean(tau)\n",
    "        if np.isscalar(thr): thr = tf.ones(n_rec, dtype=dtype) * np.mean(thr)\n",
    "        tau = tf.cast(tau, dtype=dtype)\n",
    "        dt = tf.cast(dt, dtype=dtype)\n",
    "\n",
    "        self.dampening_factor = dampening_factor\n",
    "\n",
    "        # Parameters\n",
    "        self.n_delay = n_delay\n",
    "        self.n_refractory = n_refractory\n",
    "\n",
    "        self.dt = dt\n",
    "        self.n_in = n_in\n",
    "        self.n_rec = n_rec\n",
    "        self.data_type = dtype\n",
    "\n",
    "        self._num_units = self.n_rec\n",
    "\n",
    "        # Parameters for STDP weight update\n",
    "        self.w = np.random.normal(w0, w0*STDP_dev, n_in)\n",
    "        self.w_min = np.random.normal(w_min, w_min*STDP_dev, n_in)\n",
    "        self.w_max = np.random.normal(w_max, w_max*STDP_dev, n_in)\n",
    "\n",
    "        self.a_plus = np.random.normal(a_plus, a_plus*STDP_dev, n_in)\n",
    "        self.a_plus_sign = np.full(n_in, a_plus_sign)\n",
    "        self.a_minus = np.random.normal(a_minus, a_minus*STDP_dev, n_in)\n",
    "        self.a_minus_sign = np.full(n_in, a_minus_sign)\n",
    "\n",
    "        self.b_plus = np.random.normal(b_plus, b_plus*STDP_dev, n_in)\n",
    "        self.b_plus_sign = np.full(n_in, b_plus_sign)\n",
    "        self.b_minus = np.random.normal(b_minus, b_minus*STDP_dev, n_in)\n",
    "        self.b_minus_sign = np.full(n_in, b_minus_sign)\n",
    "\n",
    "        self.c_plus = np.random.normal(c_plus, c_plus*STDP_dev, n_in)\n",
    "        self.c_plus_sign = np.full(n_in, c_plus_sign)\n",
    "        self.c_minus = np.random.normal(c_minus, c_minus*STDP_dev, n_in)\n",
    "        self.c_minus_sign = np.full(n_in, c_minus_sign)\n",
    "\n",
    "        # Check to make sure all values are non negative and below max\n",
    "        for i in range(0, self.n_in):\n",
    "            # clip weight w within bounds\n",
    "            if (self.w[i] < self.w_min[i]):\n",
    "                self.w[i] = self.w_min[i]\n",
    "            elif (self.w[i] > self.w_max[i]):\n",
    "                self.w[i] = self.w_max[i]\n",
    "            \n",
    "            # weight update variables (a,b,c)+/- < 0 --> change to = 0\n",
    "            if (self.a_plus[i] < 0):\n",
    "                self.a_plus[i] = 0\n",
    "            if (self.a_minus[i] < 0):\n",
    "                self.a_minus[i] = 0\n",
    "            if (self.b_plus[i] < 0):\n",
    "                self.b_plus[i] = 0\n",
    "            if (self.b_minus[i] < 0):\n",
    "                self.b_minus[i] = 0\n",
    "            if (self.c_plus[i] < 0):\n",
    "                self.c_plus[i] = 0\n",
    "            if (self.c_minus[i] < 0):\n",
    "                self.c_minus[i] = 0\n",
    "    \n",
    "\n",
    "\n",
    "        self.tau = tf.Variable(tau, dtype=dtype, name=\"Tau\", trainable=False) # trainable=False, set tau as a constant\n",
    "        self._decay = tf.exp(-dt / tau) # the alpha value in membrane voltage update equation\n",
    "        self.thr = tf.Variable(thr, dtype=dtype, name=\"Threshold\", trainable=False) \n",
    "\n",
    "        self.V0 = V0 # initial membrane voltage\n",
    "        self.injected_noise_current = injected_noise_current # add some random noises\n",
    "\n",
    "        self.rewiring_connectivity = rewiring_connectivity \n",
    "        self.in_neuron_sign = in_neuron_sign # input current from former layer\n",
    "        self.rec_neuron_sign = rec_neuron_sign # recurrent current from recurrent neurons of current layer\n",
    "\n",
    "        with tf.variable_scope('InputWeights'):\n",
    "\n",
    "            # Input weights\n",
    "            if 0 < rewiring_connectivity < 1:\n",
    "                # weight_sampler from rewiring tool\n",
    "                self.w_in_val, self.w_in_sign, self.w_in_var, _ = weight_sampler(n_in, n_rec, rewiring_connectivity,\n",
    "                                                                                 neuron_sign=in_neuron_sign) \n",
    "            else:\n",
    "                self.w_in_var = tf.Variable(rd.randn(n_in, n_rec) / np.sqrt(n_in), dtype=dtype, name=\"InputWeight\")\n",
    "                self.w_in_val = self.w_in_var\n",
    "\n",
    "            self.w_in_val = self.V0 * self.w_in_val\n",
    "            self.w_in_delay = tf.Variable(rd.randint(self.n_delay, size=n_in * n_rec).reshape(n_in, n_rec),\n",
    "                                          dtype=tf.int64, name=\"InDelays\", trainable=False)\n",
    "            self.W_in = weight_matrix_with_delay_dimension(self.w_in_val, self.w_in_delay, self.n_delay)\n",
    "\n",
    "        with tf.variable_scope('RecWeights'):\n",
    "            if 0 < rewiring_connectivity < 1:\n",
    "                self.w_rec_val, self.w_rec_sign, self.w_rec_var, _ = weight_sampler(n_rec, n_rec,\n",
    "                                                                                    rewiring_connectivity,\n",
    "                                                                                    neuron_sign=rec_neuron_sign)\n",
    "            else:\n",
    "                if rec_neuron_sign is not None or in_neuron_sign is not None:\n",
    "                    raise NotImplementedError('Neuron sign requested but this is only implemented with rewiring')\n",
    "                self.w_rec_var = tf.Variable(rd.randn(n_rec, n_rec) / np.sqrt(n_rec), dtype=dtype,\n",
    "                                          name='RecurrentWeight')\n",
    "                self.w_rec_val = self.w_rec_var\n",
    "\n",
    "            recurrent_disconnect_mask = np.diag(np.ones(n_rec, dtype=bool))\n",
    "\n",
    "            self.w_rec_val = self.w_rec_val * self.V0\n",
    "            self.w_rec_val = tf.where(recurrent_disconnect_mask, tf.zeros_like(self.w_rec_val),\n",
    "                                      self.w_rec_val)  # Disconnect autotapse\n",
    "            self.w_rec_delay = tf.Variable(rd.randint(self.n_delay, size=n_rec * n_rec).reshape(n_rec, n_rec),\n",
    "                                           dtype=tf.int64, name=\"RecDelays\", trainable=False)\n",
    "            self.W_rec = weight_matrix_with_delay_dimension(self.w_rec_val, self.w_rec_delay, self.n_delay)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return LIFStateTuple(v=self.n_rec,\n",
    "                             z=self.n_rec,\n",
    "                             i_future_buffer=(self.n_rec, self.n_delay),\n",
    "                             z_buffer=(self.n_rec, self.n_refractory))\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.n_rec\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return LIFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None, dtype=tf.float32):\n",
    "\n",
    "        i_future_buffer = state.i_future_buffer + einsum_bi_ijk_to_bjk(inputs, self.W_in) + einsum_bi_ijk_to_bjk(\n",
    "            state.z, self.W_rec)\n",
    "\n",
    "        new_v, new_z = self.LIF_dynamic(\n",
    "            v=state.v,\n",
    "            z=state.z,\n",
    "            z_buffer=state.z_buffer,\n",
    "            i_future_buffer=i_future_buffer)\n",
    "\n",
    "        new_z_buffer = tf_roll(state.z_buffer, new_z, axis=2)\n",
    "        new_i_future_buffer = tf_roll(i_future_buffer, axis=2)\n",
    "\n",
    "        new_state = LIFStateTuple(v=new_v,\n",
    "                                  z=new_z,\n",
    "                                  i_future_buffer=new_i_future_buffer,\n",
    "                                  z_buffer=new_z_buffer)\n",
    "        return new_z, new_state\n",
    "\n",
    "    def LIF_dynamic(self, v, z, z_buffer, i_future_buffer, thr=None, decay=None, n_refractory=None, add_current=0.):\n",
    "        \"\"\"\n",
    "        Function that generate the next spike and voltage tensor for given cell state.\n",
    "        :param v\n",
    "        :param z\n",
    "        :param z_buffer:\n",
    "        :param i_future_buffer:\n",
    "        :param thr:\n",
    "        :param decay:\n",
    "        :param n_refractory:\n",
    "        :param add_current:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.injected_noise_current > 0:\n",
    "            add_current = tf.random_normal(shape=z.shape, stddev=self.injected_noise_current) # add random noise to current\n",
    "\n",
    "        with tf.name_scope('LIFdynamic'):\n",
    "            if thr is None: thr = self.thr\n",
    "            if decay is None: decay = self._decay\n",
    "            if n_refractory is None: n_refractory = self.n_refractory\n",
    "\n",
    "            i_t = i_future_buffer[:, :, 0] + add_current\n",
    "\n",
    "            I_reset = z * thr * self.dt # thr is fixed for LIF neuron, but changable for ALIF neuron. \n",
    "\n",
    "            new_v = decay * v + (1 - decay) * i_t - I_reset # the membrane voltage at t+dt\n",
    "\n",
    "            # Spike generation\n",
    "            v_scaled = (v - thr) / thr\n",
    "\n",
    "            # new_z = differentiable_spikes(v_scaled=v_scaled)\n",
    "            new_z = SpikeFunction(v_scaled, self.dampening_factor) # update the z value\n",
    "            # return tf.identity(z_, name=\"SpikeFunction\"), grad\n",
    "\n",
    "            if n_refractory > 0:\n",
    "                is_ref = tf.greater(tf.reduce_max(z_buffer[:, :, -n_refractory:], axis=2), 0)\n",
    "                new_z = tf.where(is_ref, tf.zeros_like(new_z), new_z)\n",
    "\n",
    "            new_z = new_z * 1 / self.dt\n",
    "\n",
    "            return new_v, new_z # return the new membrane voltage, and new input spike train\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b98aa36e9225f5dd4bd905022e3163d0382783dad6e20c2e03e2a95af931253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
