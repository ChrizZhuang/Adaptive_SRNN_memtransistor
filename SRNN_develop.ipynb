{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spiking Recurrent Neural Network (SRNN) for Memtransistor Crossbar Array Simulation**\n",
    "### **Neuron**\n",
    "A neuron should be able to change its membrane voltage under different rules, and be fired when current membrane voltage exceed the threshold voltage. Once been fired, the membrane voltage should return to 0. The only difference between the three types of neuron should be the voltage changing mechanism and the rest part should be the same.\n",
    "\n",
    "    - (DONE, but need debugging) Integrate-and-fire (IF) neuron\n",
    "    - (DONE, but need debugging) Leaky integrate-and-fire (LIF) neuron\n",
    "    - (DONE, but need debugging) Adaptive leaky integrate-and-fire (ALIF) neuron\n",
    "### **Weight Update methods**\n",
    "    - (TODO) Back propogation (BP) \n",
    "    - (TODO) Back propagation through time (BPTT) \n",
    "    - (TODO) Spike timing depedent plasticity (STDP)\n",
    "### **Model**\n",
    "    - (TODO) train\n",
    "    - (TODO) predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - **Neuron.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import namedtuple\n",
    "from toolbox.tensorflow_einsums.einsum_re_written import einsum_bi_ijk_to_bjk\n",
    "from toolbox.tensorflow_utils import tf_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell = tf.keras.layers.AbstractRNNCell\n",
    "\n",
    "IFStateTuple = namedtuple('IFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "@tf.custom_gradient # Decorator to define a function with a custom gradient.\n",
    "\n",
    "# STDP should be inserted in this function!!!!!\n",
    "def SpikeFunction(v_scaled, dampening_factor):\n",
    "    z_ = tf.math.greater(v_scaled, 0.) # returns the truth/flase value of (x > y) element-wise.\n",
    "    z_ = tf.cast(z_, dtype=tf.float32) # cast z to data type of float32, i.e., true - 1.0, false - 0.0\n",
    "\n",
    "    def grad(dy): \n",
    "        # calculate the gradient for BPTT\n",
    "        dE_dz = dy # E - error\n",
    "        dz_dv_scaled = tf.math.maximum(1 - tf.abs(v_scaled), 0)\n",
    "        dz_dv_scaled *= dampening_factor # dampening_factor = 0.3 for sequantial MNIST in NIPS 2018 \n",
    "\n",
    "        dE_dv_scaled = dE_dz * dz_dv_scaled\n",
    "\n",
    "        return [dE_dv_scaled,\n",
    "                tf.zeros_like(dampening_factor)] \n",
    "\n",
    "    return tf.identity(z_, name=\"SpikeFunction\"), grad\n",
    "\n",
    "class IF(Cell):\n",
    "    def __init__(self, n_in, n_rec, thr = 0.03, dt = 1.0, n_refractory=0, \n",
    "        n_delay = 1, dampening_factor=0.3, in_neuron_sign=None, rec_neuron_sign=None, dtype=tf.float32, \n",
    "        injected_noise_current=0., v0 = 1.):\n",
    "        '''\n",
    "            :param n_refractory: number of refractory time steps - refractory time that the neuron cannot be fired again\n",
    "            :param dtype: data type of the cell tensors\n",
    "            :param n_delay: number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        '''\n",
    "        if np.isscalar(thr): thr = tf.ones(n_rec, dtype=dtype) * np.mean(thr)\n",
    "        dt = tf.cast(dt, dtype=dtype)\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_rec = n_rec\n",
    "        self.thr = tf.Variable(thr, dtype=dtype, name=\"Threshold\", trainable=False) \n",
    "        self.dt = tf.cast(dt, dtype=dtype)\n",
    "        self.n_refractory = n_refractory #　number of refractory time steps　—　refractory time that the neuron cannot be fired again\n",
    "        self.n_delay = n_delay # number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        self.dtype = dtype # dtype of neuron tensor\n",
    "        self.v0 = v0 # initial membrane voltage\n",
    "        self.dampening_factor = dampening_factor\n",
    "\n",
    "        self._num_units = self.n_rec\n",
    "        self.in_neuron_sign = in_neuron_sign # input current from former layer\n",
    "        self.rec_neuron_sign = rec_neuron_sign # recurrent current from recurrent neurons of current layer\n",
    "        self.injected_noise_current = injected_noise_current\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return IFStateTuple(v=self.n_rec,\n",
    "                            z=self.n_rec,\n",
    "                            i_future_buffer=(self.n_rec, self.n_delay),\n",
    "                            z_buffer=(self.n_rec, self.n_refractory))\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.n_rec\n",
    "\n",
    "    def zero_state(self, batch_size, dtype = tf.float32, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return IFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "        '''Convert the IF neuron to callable object'''\n",
    "        i_future_buffer = state.i_future_buffer + einsum_bi_ijk_to_bjk(inputs, self.W_in) + einsum_bi_ijk_to_bjk(\n",
    "            state.z, self.W_rec) # self.W_in and self.W_rec need to consider\n",
    "\n",
    "        new_v, new_z = self.neuronal_dynamic(\n",
    "            v=state.v,\n",
    "            z=state.z,\n",
    "            z_buffer=state.z_buffer,\n",
    "            i_future_buffer=i_future_buffer)\n",
    "\n",
    "        new_z_buffer = tf_roll(state.z_buffer, new_z, axis=2)\n",
    "        new_i_future_buffer = tf_roll(i_future_buffer, axis=2)\n",
    "\n",
    "        new_state = IFStateTuple(v=new_v,\n",
    "                                 z=new_z,\n",
    "                                 i_future_buffer=new_i_future_buffer,\n",
    "                                 z_buffer=new_z_buffer)\n",
    "        return new_z, new_state\n",
    "\n",
    "\n",
    "    def neuronal_dynamic(self, v, z, z_buffer, i_future_buffer):\n",
    "        \"\"\"\n",
    "        Function that generate the next spike and voltage tensor for given cell state.\n",
    "        :param thr - membrane threshold voltage\n",
    "        :param v - current membrane voltage\n",
    "        :param z - input spike train from previous layer at time t\n",
    "        :return: current v, z\n",
    "        \"\"\"\n",
    "\n",
    "        if self.injected_noise_current > 0:\n",
    "            add_current = tf.random.normal(shape=z.shape, stddev=self.injected_noise_current) # add random noise to current\n",
    "\n",
    "        if thr is None: thr = self.thr\n",
    "        if n_refractory is None: n_refractory = self.n_refractory\n",
    "\n",
    "        i_t = i_future_buffer[:, :, 0] + add_current\n",
    "\n",
    "        I_reset = z * thr * self.dt # thr is fixed for LIF neuron, but changable for ALIF neuron. \n",
    "\n",
    "        new_v = v + i_t - I_reset # the membrane voltage at t+dt\n",
    "\n",
    "        # Spike generation\n",
    "        v_scaled = (v - thr) / thr\n",
    "\n",
    "        new_z = SpikeFunction(v_scaled, self.dampening_factor) \n",
    "\n",
    "        if n_refractory > 0:\n",
    "            is_ref = tf.greater(tf.reduce_max(z_buffer[:, :, -n_refractory:], axis=2), 0)\n",
    "            new_z = tf.where(is_ref, tf.zeros_like(new_z), new_z)\n",
    "\n",
    "        new_z = new_z * 1 / self.dt\n",
    "\n",
    "        return new_v, new_z # return the new membrane voltage, and new input spike train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFStateTuple = namedtuple('LIFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "class LIF(IF):\n",
    "    def __init__(self, n_in, n_rec, thr = 0.03, tau = 20., dt = 1., n_refractory=0, \n",
    "                n_delay = 1, in_neuron_sign=None, rec_neuron_sign=None, dtype=tf.float32, dampening_factor=0.3,\n",
    "                injected_noise_current=0., v0=1.):\n",
    "        '''\n",
    "            :param n_refractory: number of refractory time steps - refractory time that the neuron cannot be fired again\n",
    "            :param dtype: data type of the cell tensors\n",
    "            :param n_delay: number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        '''\n",
    "        super(LIF, self).__init__(n_in = n_in, n_rec = n_rec, thr = thr, dt = dt, n_refractory = n_refractory, \n",
    "                                  n_delay = n_delay, in_neuron_sign = in_neuron_sign, \n",
    "                                  rec_neuron_sign = rec_neuron_sign, dtype = dtype, dampening_factor = dampening_factor,\n",
    "                                  injected_noise_current = injected_noise_current, v0 = v0)\n",
    "\n",
    "        if np.isscalar(tau): tau = tf.ones(n_rec, dtype=dtype) * np.mean(tau)\n",
    "        tau = tf.cast(tau, dtype=dtype)\n",
    "        \n",
    "        self.tau = tf.Variable(tau, dtype=dtype, name=\"Tau\", trainable=False)\n",
    "        self._decay = tf.exp(-dt / tau)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return LIFStateTuple(v=self.n_rec,\n",
    "                             z=self.n_rec,\n",
    "                             i_future_buffer=(self.n_rec, self.n_delay),\n",
    "                             z_buffer=(self.n_rec, self.n_refractory))\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return LIFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "    def neuronal_dynamic(self, v, z, z_buffer, i_future_buffer, thr=None, decay=None, n_refractory=None, add_current=0.):\n",
    "\n",
    "        if self.injected_noise_current > 0:\n",
    "            add_current = tf.random.normal(shape=z.shape, stddev=self.injected_noise_current) # add random noise to current\n",
    "\n",
    "        if thr is None: thr = self.thr\n",
    "        if decay is None: decay = self._decay\n",
    "        if n_refractory is None: n_refractory = self.n_refractory\n",
    "\n",
    "        i_t = i_future_buffer[:, :, 0] + add_current\n",
    "\n",
    "        I_reset = z * thr * self.dt # thr is fixed for LIF neuron, but changable for ALIF neuron. \n",
    "\n",
    "        new_v = decay * v + (1 - decay) * i_t - I_reset # the membrane voltage at t+dt\n",
    "\n",
    "        # Spike generation\n",
    "        v_scaled = (v - thr) / thr\n",
    "\n",
    "        new_z = SpikeFunction(v_scaled, self.dampening_factor) # update the z value\n",
    "\n",
    "        if n_refractory > 0:\n",
    "            is_ref = tf.greater(tf.reduce_max(z_buffer[:, :, -n_refractory:], axis=2), 0)\n",
    "            new_z = tf.where(is_ref, tf.zeros_like(new_z), new_z)\n",
    "\n",
    "        new_z = new_z * 1 / self.dt\n",
    "\n",
    "        return new_v, new_z # return the new membrane voltage, and new input spike train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIFStateTuple = namedtuple('ALIFStateTuple', ('v', 'z', 'b', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "class ALIF(LIF):\n",
    "    def __init__(self, n_in, n_rec, tau=20, thr=0.01,\n",
    "                 dt=1., n_refractory=0, dtype=tf.float32, n_delay=1,\n",
    "                 tau_adaptation=200., beta=1.6, dampening_factor=0.3,\n",
    "                 in_neuron_sign=None, rec_neuron_sign=None, injected_noise_current=0.,\n",
    "                 v0=1.):\n",
    "        \"\"\"\n",
    "        Tensorflow cell object that simulates a LIF neuron with an approximation of the spike derivatives.\n",
    "\n",
    "        :param n_in: number of input neurons\n",
    "        :param n_rec: number of recurrent neurons\n",
    "        :param tau: membrane time constant\n",
    "        :param thr: threshold voltage\n",
    "        :param dt: time step of the simulation\n",
    "        :param n_refractory: number of refractory time steps\n",
    "        :param dtype: data type of the cell tensors\n",
    "        :param n_delay: number of synaptic delay, the delay range goes from 1 to n_delay time steps\n",
    "        :param tau_adaptation: adaptation time constant for the threshold voltage\n",
    "        :param beta: amplitude of adpatation\n",
    "        :param in_neuron_sign: vector of +1, -1 to specify input neuron signs\n",
    "        :param rec_neuron_sign: same of recurrent neurons\n",
    "        :param injected_noise_current: amplitude of current noise\n",
    "        :param V0: to choose voltage unit, specify the value of V0=1 Volt in the desired unit (example V0=1000 to set voltage in millivolts)\n",
    "        \"\"\"\n",
    "\n",
    "        super(ALIF, self).__init__(n_in=n_in, n_rec=n_rec, tau=tau, thr=thr, dt=dt, n_refractory=n_refractory,\n",
    "                                   dtype=dtype, n_delay=n_delay,\n",
    "                                   dampening_factor=dampening_factor, in_neuron_sign=in_neuron_sign,\n",
    "                                   rec_neuron_sign=rec_neuron_sign,\n",
    "                                   injected_noise_current=injected_noise_current,\n",
    "                                   v0=v0)\n",
    "\n",
    "        if tau_adaptation is None: raise ValueError(\"alpha parameter for adaptive bias must be set\")\n",
    "        if beta is None: raise ValueError(\"beta parameter for adaptive bias must be set\")\n",
    "\n",
    "        self.tau_adaptation = tf.Variable(tau_adaptation, dtype=dtype, name=\"TauAdaptation\", trainable=False)\n",
    "\n",
    "        self.beta = tf.Variable(beta, dtype=dtype, name=\"Beta\", trainable=False)\n",
    "        self.decay_b = np.exp(-dt / tau_adaptation)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [self.n_rec, self.n_rec, self.n_rec]\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return ALIFStateTuple(v=self.n_rec,\n",
    "                              z=self.n_rec,\n",
    "                              b=self.n_rec,\n",
    "                              i_future_buffer=(self.n_rec, self.n_delay),\n",
    "                              z_buffer=(self.n_rec, self.n_refractory))\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        b0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return ALIFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            b=b0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None, dtype=tf.float32):\n",
    "        with tf.name_scope('ALIFcall'):\n",
    "            i_future_buffer = state.i_future_buffer + einsum_bi_ijk_to_bjk(inputs, self.W_in) + einsum_bi_ijk_to_bjk(\n",
    "                state.z, self.W_rec)\n",
    "\n",
    "            new_b = self.decay_b * state.b + (1. - self.decay_b) * state.z\n",
    "\n",
    "            thr = self.thr + new_b * self.beta * self.V0\n",
    "\n",
    "            new_v, new_z = self.LIF_dynamic(\n",
    "                v=state.v,\n",
    "                z=state.z,\n",
    "                z_buffer=state.z_buffer,\n",
    "                i_future_buffer=i_future_buffer,\n",
    "                decay=self._decay,\n",
    "                thr=thr)\n",
    "\n",
    "            new_z_buffer = tf_roll(state.z_buffer, new_z, axis=2)\n",
    "            new_i_future_buffer = tf_roll(i_future_buffer, axis=2)\n",
    "\n",
    "            new_state = ALIFStateTuple(v=new_v,\n",
    "                                       z=new_z,\n",
    "                                       b=new_b,\n",
    "                                       i_future_buffer=new_i_future_buffer,\n",
    "                                       z_buffer=new_z_buffer)\n",
    "        return [new_z, new_v, thr], new_state\n",
    "\n",
    "\n",
    "    def static_rnn_with_gradient(cell, inputs, state, loss_function, T, verbose=True):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        thr_list = []\n",
    "        state_list = []\n",
    "        z_list = []\n",
    "        v_list = []\n",
    "\n",
    "        if verbose: print('Building forward Graph...', end=' ')\n",
    "        t0 = time()\n",
    "        for t in range(T):\n",
    "            outputs, state = cell(inputs[:, t, :], state)\n",
    "            z, v, thr = outputs\n",
    "\n",
    "            z_list.append(z)\n",
    "            v_list.append(v)\n",
    "            thr_list.append(thr)\n",
    "            state_list.append(state)\n",
    "\n",
    "        zs = tf.stack(z_list, axis=1)\n",
    "        vs = tf.stack(v_list, axis=1)\n",
    "        thrs = tf.stack(thr_list, axis=1)\n",
    "        loss = loss_function(zs)\n",
    "\n",
    "        de_dz_partial = tf.gradients(loss, zs)[0]\n",
    "        if de_dz_partial is None:\n",
    "            de_dz_partial = tf.zeros_like(zs)\n",
    "            print('Warning: Partial de_dz is None')\n",
    "        print('Done in {:.2f}s'.format(time() - t0))\n",
    "\n",
    "        def namedtuple_to_list(state):\n",
    "            return list(state._asdict().values())\n",
    "\n",
    "        zero_state_as_list = cell.zero_state(batch_size, tf.float32)\n",
    "        de_dstate = namedtuple_to_list(cell.zero_state(batch_size, dtype=tf.float32))\n",
    "        g_list = []\n",
    "        if verbose: print('Building backward Graph...', end=' ')\n",
    "        t0 = time()\n",
    "        for t in np.arange(T)[::-1]:\n",
    "\n",
    "            # gradient from next state\n",
    "            if t < T - 1:\n",
    "                state = namedtuple_to_list(state_list[t])\n",
    "                next_state = namedtuple_to_list(state_list[t + 1])\n",
    "                de_dstate = tf.gradients(ys=next_state, xs=state, grad_ys=de_dstate)\n",
    "\n",
    "                for k_var, de_dvar in enumerate(de_dstate):\n",
    "                    if de_dvar is None:\n",
    "                        de_dstate[k_var] = tf.zeros_like(zero_state_as_list[k_var])\n",
    "                        print('Warning: var {} at time {} is None'.format(k_var, t))\n",
    "\n",
    "            # add the partial derivative due to current error\n",
    "            de_dstate[0] = de_dstate[0] + de_dz_partial[:, t]\n",
    "            g_list.append(de_dstate[0])\n",
    "\n",
    "        g_list = list(reversed(g_list))\n",
    "\n",
    "        gs = tf.stack(g_list, axis=1)\n",
    "        print('Done in {:.2f}s'.format(time() - t0))\n",
    "\n",
    "        return zs, vs, thrs, gs, state_list[-1]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b98aa36e9225f5dd4bd905022e3163d0382783dad6e20c2e03e2a95af931253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
