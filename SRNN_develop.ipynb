{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spiking Recurrent Neural Network (SRNN) for Memtransistor Crossbar Array Simulation**\n",
    "### **Neuron**\n",
    "A neuron should be able to change its membrane voltage under different rules, and be fired when current membrane voltage exceed the threshold voltage. Once been fired, the membrane voltage should return to 0. The only difference between the three types of neuron should be the voltage changing mechanism and the rest part should be the same.\n",
    "\n",
    "    - (DONE, but need debugging) Integrate-and-fire (IF) neuron\n",
    "    - (DONE, but need debugging) Leaky integrate-and-fire (LIF) neuron\n",
    "    - (DONE, but need debugging) Adaptive leaky integrate-and-fire (ALIF) neuron\n",
    "### **Weight Update methods**\n",
    "For the first 2 methods, i.e., BP and BPTT, the only parameters need to be specified is the loss function and the trainable variables, then optimizer from tensorflow can be applied and get the optimal weights. For STDP, there're a lot of rethinks need to be done.\n",
    "\n",
    "    - (DONE, but need debugging) Back propagation through time (BPTT) for RNN\n",
    "    - (Done, but need debugging) Spike timing depedent plasticity (STDP)\n",
    "\n",
    "### **Model**\n",
    "    - (TODO) feed-foward (include STDP weight update)\n",
    "    - (TODO) Back propagation (include BPTT)\n",
    "    - (TODO) train\n",
    "    - (TODO) predict (based on feed-forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - **Neuron.py**\n",
    "A py file that contains three kinds of neuron as objects.\n",
    "\n",
    "    - Integrate-and-fire (IF) neuron\n",
    "    - Leaky integrate-and-fire (LIF) neuron\n",
    "    - Adaptive leaky integrate-and-fire (ALIF) neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import namedtuple\n",
    "from toolbox.tensorflow_einsums.einsum_re_written import einsum_bi_ijk_to_bjk\n",
    "from toolbox.tensorflow_utils import tf_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some utility function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient for BPTT to optimize\n",
    "def SpikeFunction(v_scaled, dampening_factor):\n",
    "    z_ = tf.math.greater(v_scaled, 0.) # returns the truth/flase value of (x > y) element-wise.\n",
    "    z_ = tf.cast(z_, dtype=tf.float32) # cast z to data type of float32, i.e., true - 1.0, false - 0.0\n",
    "\n",
    "    def grad(dy): \n",
    "        # calculate the gradient for BPTT\n",
    "        dE_dz = dy # E - error\n",
    "        dz_dv_scaled = tf.math.maximum(1 - tf.abs(v_scaled), 0) # pseudo-derivative\n",
    "        dz_dv_scaled *= dampening_factor # dampening_factor = 0.3 for sequantial MNIST in NIPS 2018 \n",
    "\n",
    "        dE_dv_scaled = dE_dz * dz_dv_scaled\n",
    "\n",
    "        return [dE_dv_scaled,\n",
    "                tf.zeros_like(dampening_factor)] \n",
    "\n",
    "    return tf.identity(z_, name=\"SpikeFunction\"), grad\n",
    "\n",
    "def weight_matrix_with_delay_dimension(w, d, n_delay):\n",
    "    \"\"\"\n",
    "    Generate the tensor of shape n_in x n_out x n_delay that represents the synaptic weights with the right delays.\n",
    "\n",
    "    :param w: synaptic weight value, float tensor of shape (n_in x n_out)\n",
    "    :param d: delay number, int tensor of shape (n_in x n_out)\n",
    "    :param n_delay: number of possible delays\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with tf.name_scope('WeightDelayer'):\n",
    "        w_d_list = []\n",
    "        for kd in range(n_delay):\n",
    "            mask = tf.equal(d, kd)\n",
    "            w_d = tf.where(condition=mask, x=w, y=tf.zeros_like(w))\n",
    "            w_d_list.append(w_d)\n",
    "\n",
    "        delay_axis = len(d.shape)\n",
    "        WD = tf.stack(w_d_list, axis=delay_axis)\n",
    "\n",
    "    return WD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell = tf.keras.layers.AbstractRNNCell\n",
    "\n",
    "IFStateTuple = namedtuple('IFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "@tf.custom_gradient # Decorator to define a function with a custom gradient.\n",
    "\n",
    "class IF(Cell):\n",
    "    def __init__(self, n_in, n_rec, thr = 0.03, dt = 1.0, n_refractory=0, \n",
    "        n_delay = 1, dampening_factor=0.3, dtype=tf.float32, \n",
    "        injected_noise_current=0., v0 = 1.):\n",
    "        '''\n",
    "            :param n_refractory: number of refractory time steps - refractory time that the neuron cannot be fired again\n",
    "            :param dtype: data type of the cell tensors\n",
    "            :param n_delay: number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        '''\n",
    "        if np.isscalar(thr): thr = tf.ones(n_rec, dtype=dtype) * np.mean(thr)\n",
    "        dt = tf.cast(dt, dtype=dtype)\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_rec = n_rec\n",
    "        self.thr = tf.Variable(thr, dtype=dtype, name=\"Threshold\", trainable=False) \n",
    "        self.dt = tf.cast(dt, dtype=dtype)\n",
    "        self.n_refractory = n_refractory #　number of refractory time steps　—　refractory time that the neuron cannot be fired again\n",
    "        self.n_delay = n_delay # number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        self.dtype = dtype # dtype of neuron tensor\n",
    "        self.v0 = v0 # initial membrane voltage\n",
    "        self.dampening_factor = dampening_factor\n",
    "\n",
    "        self._num_units = self.n_rec\n",
    "        self.injected_noise_current = injected_noise_current # take device standard deviation into consideration\n",
    "\n",
    "\n",
    "        # Input weights\n",
    "        self.w_in_var = tf.Variable(np.random.randn(n_in, n_rec) / np.sqrt(n_in), dtype=dtype, name=\"InputWeight\")\n",
    "        self.w_in_val = self.w_in_var\n",
    "\n",
    "        self.w_in_val = self.v0 * self.w_in_val\n",
    "        # randomize the initial input current weight\n",
    "        self.w_in_delay = tf.Variable(np.random.randint(self.n_delay, size=n_in * n_rec).reshape(n_in, n_rec),\n",
    "                                        dtype=tf.int64, name=\"InDelays\", trainable=False) \n",
    "        self.W_in = weight_matrix_with_delay_dimension(self.w_in_val, self.w_in_delay, self.n_delay)\n",
    "\n",
    "        # randomize the initial recurrent current weight\n",
    "        self.w_rec_var = tf.Variable(np.random.randn(n_rec, n_rec) / np.sqrt(n_rec), dtype=dtype,\n",
    "                                    name='RecurrentWeight')\n",
    "        self.w_rec_val = self.w_rec_var\n",
    "\n",
    "        recurrent_disconnect_mask = np.diag(np.ones(n_rec, dtype=bool))\n",
    "\n",
    "        self.w_rec_val = self.w_rec_val * self.v0\n",
    "        self.w_rec_val = tf.where(recurrent_disconnect_mask, tf.zeros_like(self.w_rec_val),\n",
    "                                    self.w_rec_val)  # Disconnect autotapse\n",
    "        self.w_rec_delay = tf.Variable(np.random.randint(self.n_delay, size=n_rec * n_rec).reshape(n_rec, n_rec),\n",
    "                                        dtype=tf.int64, name=\"RecDelays\", trainable=False)\n",
    "        self.W_rec = weight_matrix_with_delay_dimension(self.w_rec_val, self.w_rec_delay, self.n_delay)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return IFStateTuple(v=self.n_rec,\n",
    "                            z=self.n_rec,\n",
    "                            i_future_buffer=(self.n_rec, self.n_delay), # create space for future current\n",
    "                            z_buffer=(self.n_rec, self.n_refractory)) # create space for refractory spike trains\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.n_rec\n",
    "\n",
    "    def zero_state(self, batch_size, dtype = tf.float32, n_rec=None):\n",
    "        '''Create the zero state tuple of IF neuron'''\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return IFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "\n",
    "    def neuronal_dynamic(self, v, z, z_buffer, i_future_buffer, state=IFStateTuple):\n",
    "        \"\"\"\n",
    "        Function that generate the next spike and voltage tensor for given cell state.\n",
    "        :param thr - membrane threshold voltage\n",
    "        :param v - current membrane voltage\n",
    "        :param z - input spike train from previous layer at time t\n",
    "        :return: current v, z\n",
    "        \"\"\"\n",
    "        # create current with respect to input current from previous layer and current layer of previous timestep\n",
    "        add_current = self.W_in * z + self. W_rec * state.z\n",
    "\n",
    "        # add random noise to current\n",
    "        if self.injected_noise_current > 0:\n",
    "            add_current = add_current + tf.random.normal(shape=z.shape, stddev=self.injected_noise_current) \n",
    "            \n",
    "        if thr is None: thr = self.thr\n",
    "        if n_refractory is None: n_refractory = self.n_refractory\n",
    "\n",
    "        i_t = i_future_buffer[:, :, 0] + add_current\n",
    "\n",
    "        I_reset = z * thr * self.dt # thr is fixed for IF/LIF neuron, but changable for ALIF neuron. \n",
    "\n",
    "        new_v = v + i_t - I_reset # the membrane voltage at t+dt\n",
    "\n",
    "        # Spike generation\n",
    "        v_scaled = (v - thr) / thr\n",
    "\n",
    "        new_z = SpikeFunction(v_scaled, self.dampening_factor) \n",
    "\n",
    "        if n_refractory > 0:\n",
    "            is_ref = tf.greater(tf.reduce_max(z_buffer[:, :, -n_refractory:], axis=2), 0)\n",
    "            new_z = tf.where(is_ref, tf.zeros_like(new_z), new_z)\n",
    "\n",
    "        new_z = new_z * 1 / self.dt\n",
    "\n",
    "        return new_v, new_z # return the new membrane voltage, and new input spike train\n",
    "\n",
    "    def __call__(self, inputs, state = IFStateTuple):\n",
    "        '''Convert the IF neuron to callable object'''\n",
    "        with tf.name_scope('IFcall'):\n",
    "            # self.W_in : input current weights\n",
    "            # self.W_rec : recurrent current weights\n",
    "            i_future_buffer = state.i_future_buffer + einsum_bi_ijk_to_bjk(inputs, self.W_in) + einsum_bi_ijk_to_bjk(\n",
    "                state.z, self.W_rec) \n",
    "\n",
    "            new_v, new_z = self.neuronal_dynamic(\n",
    "                v=state.v,\n",
    "                z=state.z,\n",
    "                z_buffer=state.z_buffer,\n",
    "                i_future_buffer=i_future_buffer)\n",
    "\n",
    "            new_z_buffer = tf_roll(state.z_buffer, new_z, axis=2)\n",
    "            new_i_future_buffer = tf_roll(i_future_buffer, axis=2)\n",
    "\n",
    "            new_state = IFStateTuple(v=new_v,\n",
    "                                     z=new_z,\n",
    "                                     i_future_buffer=new_i_future_buffer,\n",
    "                                     z_buffer=new_z_buffer)\n",
    "        return new_z, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFStateTuple = namedtuple('LIFStateTuple', ('v', 'z', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "class LIF(IF):\n",
    "    def __init__(self, n_in, n_rec, thr = 0.03, tau = 20., dt = 1., n_refractory=0, \n",
    "                n_delay = 1, in_neuron_sign=None, rec_neuron_sign=None, dtype=tf.float32, dampening_factor=0.3,\n",
    "                injected_noise_current=0., v0=1.):\n",
    "        '''\n",
    "            :param n_refractory: number of refractory time steps - refractory time that the neuron cannot be fired again\n",
    "            :param dtype: data type of the cell tensors\n",
    "            :param n_delay: number of synaptic delay timestep, the delay range goes from 1 to n_delay time steps\n",
    "        '''\n",
    "        super(LIF, self).__init__(n_in = n_in, n_rec = n_rec, thr = thr, dt = dt, n_refractory = n_refractory, \n",
    "                                  n_delay = n_delay, in_neuron_sign = in_neuron_sign, \n",
    "                                  rec_neuron_sign = rec_neuron_sign, dtype = dtype, dampening_factor = dampening_factor,\n",
    "                                  injected_noise_current = injected_noise_current, v0 = v0)\n",
    "\n",
    "        if np.isscalar(tau): tau = tf.ones(n_rec, dtype=dtype) * np.mean(tau)\n",
    "        tau = tf.cast(tau, dtype=dtype)\n",
    "        \n",
    "        self.tau = tf.Variable(tau, dtype=dtype, name=\"Tau\", trainable=False)\n",
    "        self._decay = tf.exp(-dt / tau)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return LIFStateTuple(v=self.n_rec,\n",
    "                             z=self.n_rec,\n",
    "                             i_future_buffer=(self.n_rec, self.n_delay),\n",
    "                             z_buffer=(self.n_rec, self.n_refractory))\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return LIFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "        '''Convert the IF neuron to callable object'''\n",
    "        with tf.name_scope('LIFcall'):\n",
    "            i_future_buffer = state.i_future_buffer + einsum_bi_ijk_to_bjk(inputs, self.W_in) + einsum_bi_ijk_to_bjk(\n",
    "                state.z, self.W_rec) # self.W_in and self.W_rec need to consider\n",
    "\n",
    "            new_v, new_z = self.neuronal_dynamic(\n",
    "                v=state.v,\n",
    "                z=state.z,\n",
    "                z_buffer=state.z_buffer,\n",
    "                i_future_buffer=i_future_buffer)\n",
    "\n",
    "            new_z_buffer = tf_roll(state.z_buffer, new_z, axis=2)\n",
    "            new_i_future_buffer = tf_roll(i_future_buffer, axis=2)\n",
    "\n",
    "            new_state = LIFStateTuple(v=new_v,\n",
    "                                      z=new_z,\n",
    "                                      i_future_buffer=new_i_future_buffer,\n",
    "                                      z_buffer=new_z_buffer)\n",
    "        return new_z, new_state\n",
    "\n",
    "    def neuronal_dynamic(self, v, z, z_buffer, i_future_buffer, thr=None, decay=None, n_refractory=None, add_current=0.):\n",
    "\n",
    "        if self.injected_noise_current > 0:\n",
    "            add_current = tf.random.normal(shape=z.shape, stddev=self.injected_noise_current) # add random noise to current\n",
    "\n",
    "        if thr is None: thr = self.thr\n",
    "        if decay is None: decay = self._decay\n",
    "        if n_refractory is None: n_refractory = self.n_refractory\n",
    "\n",
    "        i_t = i_future_buffer[:, :, 0] + add_current\n",
    "\n",
    "        I_reset = z * thr * self.dt # thr is fixed for LIF neuron, but changable for ALIF neuron. \n",
    "\n",
    "        new_v = decay * v + (1 - decay) * i_t - I_reset # the membrane voltage at t+dt\n",
    "\n",
    "        # Spike generation\n",
    "        v_scaled = (v - thr) / thr\n",
    "\n",
    "        new_z = SpikeFunction(v_scaled, self.dampening_factor) # update the z value\n",
    "\n",
    "        if n_refractory > 0:\n",
    "            is_ref = tf.greater(tf.reduce_max(z_buffer[:, :, -n_refractory:], axis=2), 0)\n",
    "            new_z = tf.where(is_ref, tf.zeros_like(new_z), new_z)\n",
    "\n",
    "        new_z = new_z * 1 / self.dt\n",
    "\n",
    "        return new_v, new_z # return the new membrane voltage, and new input spike train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIFStateTuple = namedtuple('ALIFStateTuple', ('v', 'z', 'b', 'i_future_buffer', 'z_buffer'))\n",
    "\n",
    "class ALIF(LIF):\n",
    "    def __init__(self, n_in, n_rec, tau=20, thr=0.01,\n",
    "                 dt=1., n_refractory=0, dtype=tf.float32, n_delay=1,\n",
    "                 tau_adaptation=200., beta=1.6, dampening_factor=0.3,\n",
    "                 in_neuron_sign=None, rec_neuron_sign=None, injected_noise_current=0.,\n",
    "                 v0=1.):\n",
    "        \"\"\"\n",
    "        Tensorflow cell object that simulates a LIF neuron with an approximation of the spike derivatives.\n",
    "\n",
    "        :param n_in: number of input neurons\n",
    "        :param n_rec: number of recurrent neurons\n",
    "        :param tau: membrane time constant\n",
    "        :param thr: threshold voltage\n",
    "        :param dt: time step of the simulation\n",
    "        :param n_refractory: number of refractory time steps\n",
    "        :param dtype: data type of the cell tensors\n",
    "        :param n_delay: number of synaptic delay, the delay range goes from 1 to n_delay time steps\n",
    "        :param tau_adaptation: adaptation time constant for the threshold voltage\n",
    "        :param beta: amplitude of adpatation\n",
    "        :param in_neuron_sign: vector of +1, -1 to specify input neuron signs\n",
    "        :param rec_neuron_sign: same of recurrent neurons\n",
    "        :param injected_noise_current: amplitude of current noise\n",
    "        :param V0: to choose voltage unit, specify the value of V0=1 Volt in the desired unit (example V0=1000 to set voltage in millivolts)\n",
    "        \"\"\"\n",
    "\n",
    "        super(ALIF, self).__init__(n_in=n_in, n_rec=n_rec, tau=tau, thr=thr, dt=dt, n_refractory=n_refractory,\n",
    "                                   n_delay=n_delay, dampening_factor=dampening_factor, in_neuron_sign=in_neuron_sign,\n",
    "                                   rec_neuron_sign=rec_neuron_sign,\n",
    "                                   injected_noise_current=injected_noise_current,\n",
    "                                   v0=v0, dtype=dtype)\n",
    "\n",
    "        if tau_adaptation is None: raise ValueError(\"alpha parameter for adaptive bias must be set\")\n",
    "        if beta is None: raise ValueError(\"beta parameter for adaptive bias must be set\")\n",
    "\n",
    "        self.tau_adaptation = tf.Variable(tau_adaptation, dtype=dtype, name=\"TauAdaptation\", trainable=False)\n",
    "\n",
    "        self.beta = tf.Variable(beta, dtype=dtype, name=\"Beta\", trainable=False)\n",
    "        self.decay_b = np.exp(-dt / tau_adaptation)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [self.n_rec, self.n_rec, self.n_rec]\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return ALIFStateTuple(v=self.n_rec,\n",
    "                              z=self.n_rec,\n",
    "                              b=self.n_rec,\n",
    "                              i_future_buffer=(self.n_rec, self.n_delay),\n",
    "                              z_buffer=(self.n_rec, self.n_refractory))\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "        b0 = tf.zeros(shape=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        i_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_delay), dtype=dtype)\n",
    "        z_buff0 = tf.zeros(shape=(batch_size, n_rec, self.n_refractory), dtype=dtype)\n",
    "\n",
    "        return ALIFStateTuple(\n",
    "            v=v0,\n",
    "            z=z0,\n",
    "            b=b0,\n",
    "            i_future_buffer=i_buff0,\n",
    "            z_buffer=z_buff0\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "        with tf.name_scope('ALIFcall'):\n",
    "            i_future_buffer = state.i_future_buffer + einsum_bi_ijk_to_bjk(inputs, self.W_in) + einsum_bi_ijk_to_bjk(\n",
    "                state.z, self.W_rec)\n",
    "\n",
    "            new_b = self.decay_b * state.b + (1. - self.decay_b) * state.z\n",
    "\n",
    "            thr = self.thr + new_b * self.beta * self.V0\n",
    "\n",
    "            new_v, new_z = self.neuronal_dynamic(\n",
    "                v=state.v,\n",
    "                z=state.z,\n",
    "                z_buffer=state.z_buffer,\n",
    "                i_future_buffer=i_future_buffer,\n",
    "                decay=self._decay,\n",
    "                thr=thr)\n",
    "\n",
    "            new_z_buffer = tf_roll(state.z_buffer, new_z, axis=2)\n",
    "            new_i_future_buffer = tf_roll(i_future_buffer, axis=2)\n",
    "\n",
    "            new_state = ALIFStateTuple(v=new_v,\n",
    "                                       z=new_z,\n",
    "                                       b=new_b,\n",
    "                                       i_future_buffer=new_i_future_buffer,\n",
    "                                       z_buffer=new_z_buffer)\n",
    "        return [new_z, new_v, thr], new_state\n",
    "\n",
    "\n",
    "    def static_rnn_with_gradient(cell, inputs, state, loss_function, T, verbose=True):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        thr_list = []\n",
    "        state_list = []\n",
    "        z_list = []\n",
    "        v_list = []\n",
    "\n",
    "        if verbose: print('Building forward Graph...', end=' ')\n",
    "        t0 = time()\n",
    "        for t in range(T):\n",
    "            outputs, state = cell(inputs[:, t, :], state)\n",
    "            z, v, thr = outputs\n",
    "\n",
    "            z_list.append(z)\n",
    "            v_list.append(v)\n",
    "            thr_list.append(thr)\n",
    "            state_list.append(state)\n",
    "\n",
    "        zs = tf.stack(z_list, axis=1)\n",
    "        vs = tf.stack(v_list, axis=1)\n",
    "        thrs = tf.stack(thr_list, axis=1)\n",
    "        loss = loss_function(zs)\n",
    "\n",
    "        de_dz_partial = tf.gradients(loss, zs)[0]\n",
    "        if de_dz_partial is None:\n",
    "            de_dz_partial = tf.zeros_like(zs)\n",
    "            print('Warning: Partial de_dz is None')\n",
    "        print('Done in {:.2f}s'.format(time() - t0))\n",
    "\n",
    "        def namedtuple_to_list(state):\n",
    "            return list(state._asdict().values())\n",
    "\n",
    "        zero_state_as_list = cell.zero_state(batch_size, tf.float32)\n",
    "        de_dstate = namedtuple_to_list(cell.zero_state(batch_size, dtype=tf.float32))\n",
    "        g_list = []\n",
    "        if verbose: print('Building backward Graph...', end=' ')\n",
    "        t0 = time()\n",
    "        for t in np.arange(T)[::-1]:\n",
    "\n",
    "            # gradient from next state\n",
    "            if t < T - 1:\n",
    "                state = namedtuple_to_list(state_list[t])\n",
    "                next_state = namedtuple_to_list(state_list[t + 1])\n",
    "                de_dstate = tf.gradients(ys=next_state, xs=state, grad_ys=de_dstate)\n",
    "\n",
    "                for k_var, de_dvar in enumerate(de_dstate):\n",
    "                    if de_dvar is None:\n",
    "                        de_dstate[k_var] = tf.zeros_like(zero_state_as_list[k_var])\n",
    "                        print('Warning: var {} at time {} is None'.format(k_var, t))\n",
    "\n",
    "            # add the partial derivative due to current error\n",
    "            de_dstate[0] = de_dstate[0] + de_dz_partial[:, t]\n",
    "            g_list.append(de_dstate[0])\n",
    "\n",
    "        g_list = list(reversed(g_list))\n",
    "\n",
    "        gs = tf.stack(g_list, axis=1)\n",
    "        print('Done in {:.2f}s'.format(time() - t0))\n",
    "\n",
    "        return zs, vs, thrs, gs, state_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - **Weight_Update_Methods.py**\n",
    "A py file that contains all the weight update methods.\n",
    "\n",
    "    - Back propagation through time (BPTT) for RNN\n",
    "    - Spike timing depedent plasticity (STDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_rnn_with_gradient(cell, inputs, state,  T, loss=tf.keras.losses.SparseCategoricalCrossentropy, verbose=True):\n",
    "    '''Calculate the gradient'''\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    thr_list = []\n",
    "    state_list = []\n",
    "    z_list = []\n",
    "    v_list = []\n",
    "\n",
    "    if verbose: print('Building forward Graph...', end=' ')\n",
    "    t0 = time()\n",
    "    for t in range(T):\n",
    "        outputs, state = cell(inputs[:, t, :], state)\n",
    "        z, v, thr = outputs\n",
    "\n",
    "        z_list.append(z)\n",
    "        v_list.append(v)\n",
    "        thr_list.append(thr)\n",
    "        state_list.append(state)\n",
    "\n",
    "    zs = tf.stack(z_list, axis=1)\n",
    "    vs = tf.stack(v_list, axis=1)\n",
    "    thrs = tf.stack(thr_list, axis=1)\n",
    "    loss = loss(zs)\n",
    "\n",
    "    de_dz_partial = tf.gradients(loss, zs)[0]\n",
    "    if de_dz_partial is None:\n",
    "        de_dz_partial = tf.zeros_like(zs)\n",
    "        print('Warning: Partial de_dz is None')\n",
    "    print('Done in {:.2f}s'.format(time() - t0))\n",
    "\n",
    "    def namedtuple_to_list(state):\n",
    "        return list(state._asdict().values())\n",
    "\n",
    "    zero_state_as_list = cell.zero_state(batch_size, tf.float32)\n",
    "    de_dstate = namedtuple_to_list(cell.zero_state(batch_size, dtype=tf.float32))\n",
    "    g_list = []\n",
    "    if verbose: print('Building backward Graph...', end=' ')\n",
    "    t0 = time()\n",
    "    for t in np.arange(T)[::-1]:\n",
    "\n",
    "        # gradient from next state\n",
    "        if t < T - 1:\n",
    "            state = namedtuple_to_list(state_list[t])\n",
    "            next_state = namedtuple_to_list(state_list[t + 1])\n",
    "            de_dstate = tf.gradients(ys=next_state, xs=state, grad_ys=de_dstate)\n",
    "\n",
    "            for k_var, de_dvar in enumerate(de_dstate):\n",
    "                if de_dvar is None:\n",
    "                    de_dstate[k_var] = tf.zeros_like(zero_state_as_list[k_var])\n",
    "                    print('Warning: var {} at time {} is None'.format(k_var, t))\n",
    "\n",
    "        # add the partial derivative due to current error\n",
    "        de_dstate[0] = de_dstate[0] + de_dz_partial[:, t]\n",
    "        g_list.append(de_dstate[0])\n",
    "\n",
    "    g_list = list(reversed(g_list))\n",
    "\n",
    "    gs = tf.stack(g_list, axis=1)\n",
    "    print('Done in {:.2f}s'.format(time() - t0))\n",
    "\n",
    "    return zs, vs, thrs, gs, state_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified_STDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modified_STDP_weight_update(ratio, G_min, G_max, current_weight, \n",
    "                       alpha_p, alpha_m, beta_p, beta_m, t1, t2):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        ratio: true_update_ratio/calculated_update_ratio by exponential funcion\n",
    "        G_min: minimum conductance of a device\n",
    "        G_max: maximum conductance of a device\n",
    "        current_weight: current weight matrix\n",
    "        alpha_p: alpha index for potentiation\n",
    "        alpha_m: alpha index for depression \n",
    "        beta_p: beta index for potentiation\n",
    "        beta_m: beta index for depression\n",
    "        t1: fired time of pre-synaptic neuron\n",
    "        t2: fired time of post-synaptic neuron\n",
    "    Return:\n",
    "        updated_weights\n",
    "    '''\n",
    "    if t1 > t2: # for the case of potentiation\n",
    "        delta_weight = alpha_p * np.exp(-beta_p*(current_weight-G_min)/(G_max-G_min))\n",
    "        updated_weight = current_weight + delta_weight * ratio\n",
    "\n",
    "    elif t1 < t2: # for the case of depression\n",
    "        delta_weight = -alpha_m * np.exp(-beta_m*(G_max-current_weight)/(G_max-G_min))\n",
    "        updated_weight = current_weight + delta_weight * ratio\n",
    "    else:\n",
    "        updated_weight = current_weight\n",
    "\n",
    "    return updated_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main function for sMNIST**\n",
    "\n",
    "- (Partially Done) Define some constants\n",
    "- (Done) Generate spike trains from images\n",
    "- (TODO) Define the model\n",
    "- (TODO) Fit the model by STDP and BPTT\n",
    "- (TODO) Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle, gzip\n",
    "\n",
    "\n",
    "# (TODO) - Step 1: define some constants\n",
    "\n",
    "# Constants for the model\n",
    "input_n = tf.constant(80)\n",
    "input_exitatory_n = tf.constant(60)\n",
    "input_inhibitary_n = tf.constant(20)\n",
    "hidden_n = tf.constant(220)\n",
    "hidden_LIF_n = tf.constant(120)\n",
    "hidden_ALIF_n = hidden_n - hidden_LIF_n\n",
    "output_n = tf.constant(10)\n",
    "\n",
    "# Constants for training and testing\n",
    "print(\"Please specify the constants for STDP weight update.\")\n",
    "\n",
    "training_size = tf.constant(int(input(\" Training size: \")))\n",
    "test_size = tf.constant(int(input(\" Test size: \")))\n",
    "\n",
    "# Constants for STDP\n",
    "print(\"Please specify the constants for STDP weight update.\")\n",
    "\n",
    "# potentiation\n",
    "alpha_p = tf.constant(float(input(\" alpha_p: \")))\n",
    "beta_p = tf.constant(float(input(\" beta_p: \")))\n",
    "\n",
    "# depression\n",
    "alpha_m = tf.constant(float(input(\" alpha_m: \")))\n",
    "beta_m = tf.constant(float(input(\" beta_m: \")))\n",
    "\n",
    "# device parameters\n",
    "G_max = tf.constant(float(input(\" G_max: \")))\n",
    "G_min = tf.constant(float(input(\" G_min: \")))\n",
    "\n",
    "# time constants\n",
    "t0 = 0 # initial time as 0\n",
    "dt = 1 # timestep from  NIPS 2018\n",
    "\n",
    "# ALIF/LIF neuron constants\n",
    "tau_v = 20 # time constant for voltage changing from NIPS 2018\n",
    "tau_a = 700 # time constant for adaptive threshold from NIPS 2018\n",
    "thr = 0.1 # baseline threshold of ALIF neuron/threshold of LIF neuron\n",
    "beta = 1.8 # Scaling constant of the adaptive threshold\n",
    "n_delay = 10 # maximum synaptic delay\n",
    "n_ref = 5 # number of refractory steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: read in the MNIST images and convert them to spike trains\n",
    "def load_training_set(mnist_directory, training_size):\n",
    "    \"\"\"\n",
    "    Load the MNIST digits for training. Shuffles them as well.\n",
    "\n",
    "    Inputs: \n",
    "        mnist_directory - string: the directory to store MNIST dataset\n",
    "        training_size - int : the size of training size\n",
    "    Return:\n",
    "        training_set - np.array: training_set of randomly selected images from MNIST with size of training_size\n",
    "    \"\"\"\n",
    "    # Sanity checks\n",
    "    assert(type(mnist_directory) == str), \"Parameter mnist_directory must be a string!\"\n",
    "    assert(type(training_size) == int), \"Parameter training_size must be an int!\"\n",
    "    assert(training_size >=0 and training_size <= 60000), \"Parameter training_size must >= 0 and <= 60000!\"\n",
    "\n",
    "    print(\"Loading MNIST for training (randomized)...\")\n",
    "    \n",
    "    # Update global training_set variable\n",
    "    global training_set\n",
    "    \n",
    "    # Open MNIST pickle package\n",
    "    f = gzip.open(mnist_directory, 'rb')\n",
    "    \n",
    "    # load sets using pickle\n",
    "    # encoding necessary since pickle file was created in python 2.7\n",
    "    train, valid, _ = pickle.load(f,encoding='latin1')\n",
    "    \n",
    "    # split up into corresponding sets\n",
    "    [training_set, _] = train\n",
    "    [validation_set, _] = valid\n",
    "    \n",
    "    # update training set\n",
    "    training_set = np.concatenate((training_set, validation_set))\n",
    "    \n",
    "    # close MNIST file\n",
    "    f.close()\n",
    "    \n",
    "    # randomize loaded MNIST set\n",
    "    np.random.shuffle(training_set)\n",
    "\n",
    "    #random_index = np.random.choice(training_set)\n",
    "    random_index = np.random.choice(training_set.shape[0],training_size,replace=False)\n",
    "    training_set = training_set[random_index]\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "    return training_set\n",
    "\n",
    "\n",
    "# Reference repo: \n",
    "# https://github.com/intel-nrc-ecosystem/models/blob/master/nxsdk_modules_ncl/lsnn/tutorials/smnist_tutorial.ipynb\n",
    "# Part: Encoding the images into spikes\n",
    "# spike encoding example\n",
    "def find_onset_offset(y, threshold):\n",
    "    \"\"\"\n",
    "    Given the input signal y with samples,\n",
    "    find the indices where y increases and descreases through the value threshold.\n",
    "    Return stacked binary arrays of shape y indicating onset and offset threshold crossings.\n",
    "    y must be 1-D numpy arrays.\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    #assert (len(np.array(threshold)) == 1), \"The length of threshold should be 1!\"\n",
    "\n",
    "    if threshold == 255:\n",
    "        equal = y == threshold\n",
    "        transition_touch = np.where(equal)[0]\n",
    "        touch_spikes = np.zeros_like(y)\n",
    "        touch_spikes[transition_touch] = 1\n",
    "        return np.expand_dims(touch_spikes, axis=0)\n",
    "    else:\n",
    "        # Find where y crosses the threshold (increasing).\n",
    "        lower = y < threshold\n",
    "        higher = y >= threshold\n",
    "        transition_onset = np.where(lower[:-1] & higher[1:])[0]\n",
    "        transition_offset = np.where(higher[:-1] & lower[1:])[0]\n",
    "        onset_spikes = np.zeros_like(y)\n",
    "        offset_spikes = np.zeros_like(y)\n",
    "        onset_spikes[transition_onset] = 1\n",
    "        offset_spikes[transition_offset] = 1\n",
    "\n",
    "    return np.stack((onset_spikes, offset_spikes))\n",
    "\n",
    "\n",
    "def generate_spike_train_from_image(image, n_inputs = 80):\n",
    "    \"\"\"Generate spike trains from an given image.\n",
    "    Input:\n",
    "        image: a numpy array that represent pixels of an image\n",
    "        n_inputs: number of input neurons.\n",
    "        n_thresholds: number of thresholds for the input layer.\n",
    "                      should be around half of the number of input neurons. \n",
    "    Output\n",
    "        spike_train_image: spike train of a given image\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    assert(type(image) == np.ndarray), \"Parameter image should be a np.array!\"\n",
    "    assert(type(n_inputs) == int), \"Parameter n_inputs should be an int!\"\n",
    "\n",
    "    # turn the image into a 1D array\n",
    "    image = image.reshape(-1, 1)\n",
    "\n",
    "    thresholds = np.linspace(0, 255, n_inputs // 2)\n",
    "\n",
    "    spike_train_image = []\n",
    "    for pixel in image:  # shape img = (784)\n",
    "        Sspikes = None\n",
    "        for thr in thresholds:\n",
    "            if Sspikes is not None:\n",
    "                Sspikes = np.concatenate((Sspikes, find_onset_offset(pixel, thr)))\n",
    "            else:\n",
    "                Sspikes = find_onset_offset(pixel, thr)\n",
    "        Sspikes = np.array(Sspikes)  # shape Sspikes = (31, 784)\n",
    "        Sspikes = np.swapaxes(Sspikes, 0, 1)\n",
    "        spike_train_image.append(Sspikes)\n",
    "    spike_train_image = np.array(spike_train_image)\n",
    "    # add output cue neuron, and expand time for two image rows (2*28)\n",
    "    out_cue_duration = 2*28 # 56 timesptes to mark the end of encoding an image\n",
    "    spike_train_image = np.lib.pad(spike_train_image, ((0, 0), (0, out_cue_duration), (0, 1)), 'constant')\n",
    "    # output cue neuron fires constantly for these additional recall steps\n",
    "    spike_train_image[:, -out_cue_duration:, -1] = 1\n",
    "\n",
    "    return spike_train_image\n",
    "\n",
    "def image_process_pipeline(mnist_directory, training_size):\n",
    "    training_set = load_training_set(mnist_directory, training_size)\n",
    "    spike_trains_training_set = []\n",
    "    for image in training_set:\n",
    "        spike_trains_training_set.append(generate_spike_train_from_image(image))\n",
    "\n",
    "    return np.array(spike_trains_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TODO) - Step 3: define the model\n",
    "beta = np.concatenate([np.zeros(hidden_LIF_n), np.ones(hidden_ALIF_n) * beta])\n",
    "cell = ALIF(n_in=input_n, n_rec=hidden_LIF_n + hidden_ALIF_n, tau=tau_v, thr=thr, dt=dt,\n",
    "                n_refractory=n_ref, dtype = tf.float32, tau_adaptation=tau_a, beta=beta, thr=thr,\n",
    "                dampening_factor=FLAGS.dampening_factor,\n",
    "                )\n",
    "\n",
    "        super(ALIF, self).__init__(n_in=n_in, n_rec=n_rec, tau=tau, thr=thr, dt=dt, n_refractory=n_refractory,\n",
    "                                   dtype=dtype, n_delay=n_delay,\n",
    "                                   dampening_factor=dampening_factor, in_neuron_sign=in_neuron_sign,\n",
    "                                   rec_neuron_sign=rec_neuron_sign,\n",
    "                                   injected_noise_current=injected_noise_current,\n",
    "                                   v0=v0)\n",
    "# (TODO) - Step 4: fit the model\n",
    "# (TODO) - Step 5: test and calculate the accuracy  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b98aa36e9225f5dd4bd905022e3163d0382783dad6e20c2e03e2a95af931253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
